<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Haozhe Du</title>

    <meta name="author" content="Haozhe Du">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <script type="text/javascript" src="js/hidebib.js"></script>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-XXXXX-Y', 'auto');
      ga('send', 'pageview');
      </script>
    <!-- End : Google Analytics Code -->
    <!-- Scramble Script by Jeff Donahue -->
    <script src="js/scramble.js"></script>
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Haozhe Du 杜浩哲
                </p>
                <p>
                I'm currently a Master student in <a href="https://github.com/ZJU-Robotics-Lab">Robotics Lab</a> at Zhejiang University, advised by  <a href="https://scholar.google.com/citations?user=1hI9bqUAAAAJ&hl=en">Prof. Rong Xiong</a>. Previously, I received my B.S. degree in automation and also a dual degree in mechatronic engineering from <a href="http://ckc.zju.edu.cn/">Chu Kochen Honors College</a>, Zhejiang University.
                </p>
                <p>
                My current research topic lies in robot learning and robot manipulation. I am interested in designing generalizable algorithms for state estimation, dynamics modeling, and planning for deformable object manipulation. Previously I focus on motion prediction and decision making for robot swarm. My long-term goal is to build robust, generalizable autonomous robot agents that can achieve and surpass human performance.
                </p>

                <p style="text-align:center">
                  <a href="hzdu0915@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data1/Haozhe_Du_CV_github.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="data1/Haozhe_Du-bio.txt">Bio</a> &nbsp;/&nbsp;
                  <!-- <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Scholar</a> &nbsp;/&nbsp; -->
                  <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp; -->
                  <a href="https://github.com/LeoDuhz/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="data1/Haozhe_Du.jpg"><img style="width:100%;max-width:100%;object-fit: cover;" alt="profile photo" src="data1/Haozhe_Du.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>




    <tr onmouseout="polyfold_stop()" onmouseover="polyfold_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='polyfold_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images1/polyfold.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images1/polyfold.png' width="160">
        </div>
        <script type="text/javascript">
          function polyfold_start() {
            document.getElementById('polyfold_image').style.opacity = "1";
          }

          function polyfold_stop() {
            document.getElementById('polyfold_image').style.opacity = "0";
          }
          polyfold_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://sites.google.com/view/polyfold">
			<span class="papertitle"> PolyFold: A Generalizable Framework for Language-Conditioned Bimanual Cloth Folding
</span>
        </a>
        <br>
				<strong>Haozhe Du</strong>,
        Rong Xiong,
        Yue Wang
        <br>
        Under Review
        <br>
        <div class="paper" id="polyfold">
        <a href="https://sites.google.com/view/polyfold">project page</a> /
        <a href="javascript:toggleblock('polyfold_abs')">abstract</a> 
        <!-- <a shape="rect" href="javascript:togglebib('polyfold')" class="togglebib">bibtex</a> | -->
        <p align="justify"> <i id="polyfold_abs">Cloth folding stands as an intricate subject in robot manipulation, requiring robots to fold diverse fabrics into different configurations according to human intentions. Most previous approaches address this problem in a vision/language goal-conditioned way. They usually depend on precise subgoals and lack inherent multi-step reasoning ability. They also require substantial training data from expert demonstrations and often face difficulties generalizing to different, previously unseen cloth appearances and tasks. To tackle these problems, we present PolyFold, a novel language-conditioned bimanual cloth folding framework that excels in zero-shot generalization and inherent multi-step reasoning capability, while also operating in an expert-demonstration-free manner. Given solely a language goal (even if it is ambiguous), PolyFold utilizes parameterized polygon model, Large Language Models (LLMs), and a self-supervised learning downstream policy to achieve a sequence of successful cloth folding actions. PolyFold can be divided into three key parts: subgoal decomposition LLM, symmetrical fold line generation LLM, and fold line guided bimanual pick-and-place policy. Experiments show that PolyFold is able to zero-shot generalize to various cloth types and 70 cloth folding tasks when providing only final language instructions, surpassing previous SOTA vision-conditioned and language-conditioned methods. Our method can also be directly deployed in real-world scenarios. </i></p>
      </div>

        <p></p>
        <p>
				Generalizable bimanual cloth folding framework that listens to user language instructions and achieves successful folding results on unseen cloth and unseen tasks.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images1/smt.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://hz-du.github.io/">
      <span class="papertitle"> Semantic Mask Transformer for 3D Human Pose Generation with Detailed Text Description</span>
    </a>
    <br>
        Zhike Chen, 
        <strong>Haozhe Du</strong>,
        Haodong Zhang,
        Rong Xiong
        <br>
        in submission to AAAI 2025, under review<br>
        </p>
        <div class="paper" id="smt">
        <!-- <a href="https://ieeexplore.ieee.org/document/9517404/">pdf</a> / -->
        <a href="javascript:toggleblock('smt_abs')">abstract</a> 
        <!-- <a shape="rect" href="javascript:togglebib('gnn_rcar')" class="togglebib">bibtex</a> / -->
        <!-- <a href="https://github.com/LeoDuhz/MultiAgentTrajPrediction">code</a> -->

        <p align="justify"> <i id="smt_abs">Previous methods for generating 3D human poses from detailed text descriptions often encounter semantic mismatches and struggle to produce precise local body part actions. These challenges primarily arise from the limited variety of body part action combinations in existing datasets. To address these issues, we introduce the Semantic Mask Transformer (SMT), a text-driven animation framework designed to synthesize 3D poses that closely align with detailed textual descriptions. The key innovation of SMT lies in its integration of semantic biases derived from a Large Language Model into the training objectives, thereby enhancing local semantic consistency. Equipped with mask data augmentation, body part modeling, and semantic bias training objectives, our SMT effectively generates high-quality poses while maintaining accurate semantic alignment with the input descriptions. Furthermore, the ablation study demonstrates that the semantic bias objectives can be applied across various backbone architectures.</i></p>

        <!-- <pre xml:space="preserve">
          @inproceedings{du2021multi,
            title={Multi-agent trajectory prediction based on graph neural network},
            author={Du, Haozhe and Chen, Zhike and Wang, Yufeng and Huang, Zheyuan and Wang, Yunkai and Xiong, Rong},
            booktitle={2021 IEEE International Conference on Real-time Computing and Robotics (RCAR)},
            pages={632--637},
            year={2021},
            organization={IEEE}
          }
        </pre> -->
      </div>

        <p></p>
        <p>
        Integrating semantic biases derived from a Large Language Model into the training objectives to enhance local semantic consistency.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images1/DPCN++.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/document/10256027">
      <span class="papertitle">DPCN++: Differentiable Phase Correlation Network for Versatile Pose Registration</span>
    </a>
    <br>
    Zexi Chen, Yiyi Liao, <strong>Haozhe Du</strong>, Haodong Zhang, Xuecheng Xu, Haojian Lu, Rong Xiong, and Yue Wang.
        <br>
        IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 2023.<br>
        </p>
        <div class="paper" id="DPCN++">
        <a href="https://ieeexplore.ieee.org/document/9517404/">pdf</a> /
        <a href="https://arxiv.org/pdf/2206.05707">arxiv</a> /
        <a href="javascript:toggleblock('DPCN++_abs')">abstract</a> /
        <a shape="rect" href="javascript:togglebib('DPCN++')" class="togglebib">bibtex</a> 

        <p align="justify"> <i id="DPCN++_abs">Pose registration is critical in vision and robotics. This paper focuses on the challenging task of initialization-free pose registration up to 7DoF for homogeneous and heterogeneous measurements. While recent learning-based methods show promise using differentiable solvers, they either rely on heuristically defined correspondences or are prone to local minima. We present a differentiable phase correlation (DPC) solver that is globally convergent and correspondence-free. When combined with simple feature extraction networks, our general framework DPCN++ allows for versatile pose registration with arbitrary initialization. Specifically, the feature extraction networks first learn dense feature grids from a pair of homogeneous/heterogeneous measurements. These feature grids are then transformed into a translation and scale invariant spectrum representation based on Fourier transform and spherical radial aggregation, decoupling translation and scale from rotation. Next, the rotation, scale, and translation are independently and efficiently estimated in the spectrum step-by-step using the DPC solver. The entire pipeline is differentiable and trained end-to-end. We evaluate DCPN++ on a wide range of registration tasks taking different input modalities, including 2D bird's-eye view images, 3D object and scene measurements, and medical images. Experimental results demonstrate that DCPN++ outperforms both classical and learning-based baselines, especially on partially observed and heterogeneous measurements.</i></p>

        <pre xml:space="preserve">
          @article{chen2023dpcn++,
            title={DPCN++: Differentiable Phase Correlation Network for Versatile Pose Registration},
            author={Chen, Zexi and Liao, Yiyi and Du, Haozhe and Zhang, Haodong and Xu, Xuecheng and Lu, Haojian and Xiong, Rong and Wang, Yue},
            journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
            year={2023},
            publisher={IEEE}
          }
        </pre>
      </div>

        <p></p>
        <p>
        Differentiable phase correlation (DPC) solver for initialization-free up to 7DoF pose registration of homogeneous and heterogeneous measurements.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images1/BEVIO.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://proceedings.mlr.press/v205/chen23c/chen23c.pdf">
      <span class="papertitle">Learning Interpretable BEV Based VIO without Deep Neural Networks</span>
    </a>
    <br>
    Zexi Chen, <strong>Haozhe Du</strong>, Xuecheng Xu, Rong Xiong, Yiyi Liao, Yue Wang.
        <br>
        Conference on Robot Learning (CoRL) 2023.<br>
        </p>
        <div class="paper" id="BEVIO">
        <a href="https://proceedings.mlr.press/v205/chen23c/chen23c.pdf">pdf</a> /
        <a href="https://arxiv.org/abs/2109.12292">arxiv</a> /
        <a href="javascript:toggleblock('BEVIO_abs')">abstract</a> /
        <a shape="rect" href="javascript:togglebib('BEVIO')" class="togglebib">bibtex</a> 

        <p align="justify"> <i id="BEVIO_abs"> Monocular visual-inertial odometry (VIO) is a critical problem in robotics and autonomous driving. Traditional methods solve this problem based on filtering or optimization. While being fully interpretable, they rely on manual interference and empirical parameter tuning. On the other hand, learning-based approaches allow for end-to-end training but require a large number of training data to learn millions of parameters. However, the non-interpretable and heavy models hinder the generalization ability. In this paper, we propose a fully differentiable, and interpretable, bird-eye-view (BEV) based VIO model for robots with local planar motion that can be trained without deep neural networks. Specifically, we first adopt Unscented Kalman Filter as a differentiable layer to predict the pitch and roll, where the covariance matrices of noise are learned to filter out the noise of the IMU raw data. Second, the refined pitch and roll are adopted to retrieve a gravity-aligned BEV image of each frame using differentiable camera projection. Finally, a differentiable pose estimator is utilized to estimate the remaining 3 DoF poses between the BEV frames: leading to a 5 DoF pose estimation. Our method allows for learning the covariance matrices end-to-end supervised by the pose estimation loss, demonstrating superior performance to empirical baselines. Experimental results on synthetic and real-world datasets demonstrate that our simple approach is competitive with state-of-the-art methods and generalizes well on unseen scenes.</i></p>

        <pre xml:space="preserve">
          @inproceedings{chen2023learning,
            title={Learning interpretable BEV based VIO without deep neural networks},
            author={Chen, Zexi and Du, Haozhe and Xuecheng, XU and Xiong, Rong and Liao, Yiyi and Wang, Yue},
            booktitle={Conference on Robot Learning},
            pages={1289--1298},
            year={2023},
            organization={PMLR}
          }
        </pre>
      </div>

        <p></p>
        <p>
        Fully differentiable, and interpretable bird-eye-view (BEV) based VIO model.
        </p>
      </td>
    </tr>

    <tr onmouseout="gnn_rcar_stop()" onmouseover="gnn_rcar_start()"></tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
            <img src='images1/gnn_rcar.png' width="160">
        </div>
        
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/document/9517404/">
          <span class="papertitle">Multi-Agent Trajectory Prediction Based on Graph Neural Network</span>
        </a>
        <br>
        <strong>Haozhe Du</strong>,
        Zhike Chen,
        Yufeng Wang,
        Zheyuan Huang,
        Yunkai Wang,
        Rong Xiong.
        <br>
        <em>IEEE International Conference on Real-time Computing and Robotics (RCAR)</em>, 2021
        <br>
        <div class="paper" id="gnn_rcar">
        <a href="https://ieeexplore.ieee.org/document/9517404/">pdf</a> /
        <a href="javascript:toggleblock('gnn_rcar_abs')">abstract</a> /
        <a shape="rect" href="javascript:togglebib('gnn_rcar')" class="togglebib">bibtex</a> /
        <a href="https://github.com/LeoDuhz/MultiAgentTrajPrediction">code</a>

        <p align="justify"> <i id="gnn_rcar_abs">Many tasks have demand on precise predictions of agents or moving objects. Previous prediction methods usually only focus on the kinematic model of moving objects or the environment. However, the target tasks of agents may influence the prediction of agents in great sense, especially in tasks of confrontation. Therefore traditional methods cannot work well in such scenes. In this paper, we propose a heterogeneous graph neural network method to deal with the multi-agent trajectory prediction problem. Our method can aggregate and pass messages representing environment and also agents' tasks due to the elaborate design of graph neural network structure. We validate our method on the Robocup Small Size League simulation platform which focuses on multi-agent coordination and confrontation in the form of soccer games. After making our own ZJUNlictSSL dataset, we predict the position of all robots on the pitch of certain time gaps based on the limited information we get from vision. The results prove that our method is of high prediction accuracy and low prediction error compared with conventional kinematic motion methods.</i></p>

        <pre xml:space="preserve">
          @inproceedings{du2021multi,
            title={Multi-agent trajectory prediction based on graph neural network},
            author={Du, Haozhe and Chen, Zhike and Wang, Yufeng and Huang, Zheyuan and Wang, Yunkai and Xiong, Rong},
            booktitle={2021 IEEE International Conference on Real-time Computing and Robotics (RCAR)},
            pages={632--637},
            year={2021},
            organization={IEEE}
          }
        </pre>
      </div>

        <p></p>
        <p>
        Heterogeneous graph neural network for robot motion prediction in soccer robot platform that features strong confrontation and cooperation.
        </p>
      </td>
    </tr>


<!-- Project -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
  <tr>
    <td>
      <h2>Project</h2>
    </td>
  </tr>
</tbody></table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
          <img src='images1/gnn_rcar.png' width="160">
      </div>
      
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
<a href="https://hz-du.github.io/" id="messidecision">
  <span class="papertitle">Decision Making System for Soccer Robot Swarm (Robocup Small Size League)</span></a><br>

      <div class="paper" id="messidecision">
      <a href="https://www.bilibili.com/video/BV18p4y1r7Vm/?vd_source=105cdc9a3f67adb818b7b6cab3dcb695"> video (bilibili)</a> /
      <a href="https://github.com/Robocup-ssl-China/rocos"> open source project</a> /
      <a href="https://ssl.robocup.org/wp-content/uploads/2023/02/2023_ETDP_ZJUNlict.pdf"> team description paper</a>

      <p>Centralized and hierarchical decision making system for soccer robot swarm, which can be divided into three parts: skill, tactic and play. Skill: single robot skill, such as passing, tacking, running. Tactic: value-function based future robot position calculation. Play: general state calculation (offense, defense), role and task assignment.   </p>

      </div>
    </td>
  </tr>
  <!-- <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <iframe src="https://drive.google.com/file/d/1w3UesUsZKPBk_8esqDOMvup3mIoHPLUW/preview" 
        width="960" 
        height="540" 
        allow="autoplay" 
        allowfullscreen>
      </iframe>
    </td>
  </tr> -->
</tbody>
</table>-



<!-- Education -->
<br>
<hr class="soft">
<br>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
  <tr>
    <td>
      <h2>Education</h2>
    </td>
  </tr>
</tbody></table>
<table width="100%" align="center" border="0" cellpadding="20"><tbody>
  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <img src="images1/Zhejiang_University_Logo.svg", width="160"></td>

      <td style="padding:10px;width:75%;vertical-align:top">
        <!-- <papertitle style="color:gray"><big>Software Development Engineer Intern</big> </papertitle> <papertitle ><big> | Expedia Group</big></papertitle> -->
        <p style="font-size: 20px;">Zhejiang University</p>
        <!-- <br> -->
        <papertitle><big>Master in Control Science and Engineering</big></papertitle>
        <br>
        September 2022 - March 2025 (Expected)
        <!-- <br>  -->
        <!-- <br> -->
        <p>
            <!-- Research: -->
            <!-- <ul> -->
              <li>Working as a graduate researcher at <a href="https://github.com/ZJU-Robotics-Lab">Robotics Lab</a>, advised by Prof.Rong Xiong </li>
          
            <!-- </ul> -->
        </p>

        <papertitle><big>Bachelor in Automation (Robotics Track)</big></papertitle>
        <br>
        September 2018 - June 2022
        <!-- <br>  -->
        <!-- <br> -->
        <p>
            <!-- Research: -->
            <!-- <ul> -->
              <li>GPA: 3.97/4.00, 91.10/100, Rank:1/28</li>
              <li>Dual bachelor degree in mechatronic engineering.</li>
              <li>Core team member of ZJUNlict, a Robocup Small Size League team in Zhejiang University.</li>
            <!-- </ul> -->
        </p>
      </td>

  </tr>

</tbody></table>
<br>
<hr class="soft">
<br>

 <!-- Mischaellanea -->
 <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
  <tr>
    <td>
      <h2>Miscellanea</h2>
    </td>
  </tr>
</tbody></table>
<table width="100%" align="center" border="0" cellpadding="20"><tbody>
  <tr>
      <td style="padding:10px;width:75%;vertical-align:top;">
        <li style="margin-left: 40px; margin-top: -20px; font-size: 15px;">
          I love sports, including soccer, basketball, running and badminton. My favorite soccer player is Lionel Andrés Messi, and my favorite soccer club is FC Barcelona. Currently I usually pay attention to the performance of Pedri, Jamal Musiala, Gavi, Lamine Yamal and Pau Cubarsí.
          </li>
          <li style="margin-left: 40px; margin-top: 0px; font-size: 15px;">
            I love eFootball™ (previously called PES) game by KONAMI and I have been playing it since my senior high school. You are always welcome to have a PVP game with me.</li>
          <li style="margin-left: 40px; margin-top: 0px; font-size: 15px;">
            I enjoy reading books across various topics, especially philosophy though I am still a beginner.</li>
          <li style="margin-left: 40px; margin-top: 0px; font-size: 15px;">
            I love music and I used to play clarinet. My favorite singers are Mao Buyi, Li Jian, Chen Chusheng, and Yang Zongwei.</li>
      </td>

  </tr>
  </tbody>
</table> -->



<!-- google maps -->
<table width="40%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody>
      <tr>
          <td style="padding:0px">
              <br>
              <br>
              <div>
                <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=92c8PfgYeaOVI8ywx2D-FuKnadIu357Pczo30Xp_kh8&cl=ffffff&w=a"></script>
                  <!-- <a target="_top" href="http://clustrmaps.com/site/1acpn?utm_source=widget&amp;utm_campaign=widget_ctr" id="clustrmaps-widget-v2" class="clustrmaps-map-control" style="width: 300px;">-->                               
              </div>
          </td>
      </tr>
  </tbody>
</table>
            
<!-- website template -->
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Website Template from <a href="https://jonbarron.info/">https://jonbarron.info/</a>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  

    <script xml:space="preserve" language="JavaScript">
      hideallbibs();
    </script>
    <script xml:space="preserve" language="JavaScript">
      hideblock('polyfold_abs');
    </script>
    <script xml:space="preserve" language="JavaScript">
      hideblock('smt_abs');
    </script>
    <script xml:space="preserve" language="JavaScript">
      hideblock('DPCN++_abs');
    </script>
    <script xml:space="preserve" language="JavaScript">
      hideblock('BEVIO_abs');
    </script>
    <script xml:space="preserve" language="JavaScript">
      hideblock('gnn_rcar_abs');
    </script>
  
  </body>
</html>
